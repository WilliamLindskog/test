{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15871,
     "status": "ok",
     "timestamp": 1670356049976,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "2c588ea0-a383-4461-e633-794e73d0f57a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./processed_heart_disease.csv'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import bz2\n",
    "import shutil\n",
    "\n",
    "CLASSIFICATION_PATH = './processed_heart_disease.csv'\n",
    "REGRESSION_PATH = ''\n",
    "\n",
    "CLASSIFICATION_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1670356049977,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "5289e33e-e18e-491b-d536-6b1052598994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported modules.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchmetrics import Accuracy, MeanSquaredError\n",
    "from tqdm import trange, tqdm\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Flower relevant modules for Federated XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported modules.\n"
     ]
    }
   ],
   "source": [
    "import flwr as fl\n",
    "from flwr.common.typing import Parameters\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from flwr.common import NDArray, NDArrays\n",
    "\n",
    "print(\"Imported modules.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility function for xgboost trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt  # pylint: disable=E0401\n",
    "\n",
    "\n",
    "def plot_xgbtree(tree: Union[XGBClassifier, XGBRegressor], n_tree: int) -> None:\n",
    "    \"\"\"Visualize the built xgboost tree.\"\"\"\n",
    "    xgb.plot_tree(tree, num_trees=n_tree)\n",
    "    plt.rcParams[\"figure.figsize\"] = [50, 10]\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def construct_tree(\n",
    "    dataset: Dataset, label: NDArray, n_estimators: int, tree_type: str\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree form tabular dataset.\"\"\"\n",
    "    if tree_type == \"BINARY\":\n",
    "        tree = xgb.XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=0.8,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=1,\n",
    "            alpha=5,\n",
    "            gamma=5,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=1,\n",
    "        )\n",
    "\n",
    "    elif tree_type == \"REG\":\n",
    "        tree = xgb.XGBRegressor(\n",
    "            objective=\"reg:squarederror\",\n",
    "            learning_rate=0.1,\n",
    "            max_depth=8,\n",
    "            n_estimators=n_estimators,\n",
    "            subsample=0.8,\n",
    "            colsample_bylevel=1,\n",
    "            colsample_bynode=1,\n",
    "            colsample_bytree=1,\n",
    "            alpha=5,\n",
    "            gamma=5,\n",
    "            num_parallel_tree=1,\n",
    "            min_child_weight=1,\n",
    "        )\n",
    "\n",
    "    tree.fit(dataset, label)\n",
    "    return tree\n",
    "\n",
    "\n",
    "def construct_tree_from_loader(\n",
    "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
    ") -> Union[XGBClassifier, XGBRegressor]:\n",
    "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
    "    for dataset in dataset_loader:\n",
    "        data, label = dataset[0], dataset[1]\n",
    "    return construct_tree(data, label, n_estimators, tree_type)\n",
    "\n",
    "\n",
    "def single_tree_prediction(\n",
    "    tree: Union[XGBClassifier, XGBRegressor], n_tree: int, dataset: NDArray\n",
    ") -> Optional[NDArray]:\n",
    "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
    "    ensemble.\"\"\"\n",
    "    # How to access a single tree\n",
    "    # https://github.com/bmreiniger/datascience.stackexchange/blob/master/57905.ipynb\n",
    "    num_t = len(tree.get_booster().get_dump())\n",
    "    if n_tree > num_t:\n",
    "        print(\n",
    "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    return tree.predict(  # type: ignore\n",
    "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
    "    )\n",
    "\n",
    "\n",
    "def tree_encoding(  # pylint: disable=R0914\n",
    "    trainloader: DataLoader,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Optional[Tuple[NDArray, NDArray]]:\n",
    "    \"\"\"Transform the tabular dataset into prediction results using the\n",
    "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
    "    if trainloader is None:\n",
    "        return None\n",
    "\n",
    "    for local_dataset in trainloader:\n",
    "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
    "\n",
    "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
    "    x_train_enc = np.array(x_train_enc, copy=True)\n",
    "\n",
    "    temp_trees: Any = None\n",
    "    if isinstance(client_trees, list) is False:\n",
    "        temp_trees = [client_trees[0]] * client_num\n",
    "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
    "        temp_trees = [client_trees[0][0]] * client_num\n",
    "    else:\n",
    "        cids = []\n",
    "        temp_trees = []\n",
    "        for i, _ in enumerate(client_trees):\n",
    "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
    "            cids.append(client_trees[i][1])  # type: ignore\n",
    "        sorted_index = np.argsort(np.asarray(cids))\n",
    "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
    "\n",
    "    for i, _ in enumerate(temp_trees):\n",
    "        for j in range(client_tree_num):\n",
    "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
    "                temp_trees[i], j, x_train\n",
    "            )\n",
    "\n",
    "    x_train_enc32: Any = np.float32(x_train_enc)\n",
    "    y_train32: Any = np.float32(y_train)\n",
    "\n",
    "    x_train_enc32, y_train32 = torch.from_numpy(\n",
    "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
    "    ), torch.from_numpy(\n",
    "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
    "    )\n",
    "    return x_train_enc32, y_train32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually download and load the tabular dataset from LIBSVM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(920, 15)\n",
      "(299, 15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "num\n",
       "0    160\n",
       "1    139\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(CLASSIFICATION_PATH)\n",
    "# drop rows with ? in them\n",
    "print(df.shape)\n",
    "for row in df.index:\n",
    "    if '?' in df.loc[row].values:\n",
    "        df.drop(row, inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# change values in num that are > 1 to 1 \n",
    "df.loc[df['num'] > 1, 'num'] = 1\n",
    "df['num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26613,
     "status": "ok",
     "timestamp": 1670356076585,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "22843504-faf0-44cf-aedd-1df8d0ec87a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         float64\n",
      "sex         float64\n",
      "cp          float64\n",
      "trestbps     object\n",
      "chol         object\n",
      "fbs          object\n",
      "restecg      object\n",
      "thalach      object\n",
      "exang        object\n",
      "oldpeak      object\n",
      "slope        object\n",
      "ca           object\n",
      "thal         object\n",
      "num           int64\n",
      "region       object\n",
      "dtype: object\n",
      "Task type selected is: BINARY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "# set all except region to int \n",
    "df['region'] = df['region'].astype('category')\n",
    "df['region'] = df['region'].cat.codes\n",
    "\n",
    "# set all except region to int\n",
    "for col in df.columns:\n",
    "    if col not in ['region', 'age', 'sex', 'cp']:\n",
    "        try:\n",
    "            df[col] = df[col].astype('int64')\n",
    "        except:\n",
    "            df[col] = df[col].astype('float64')\n",
    "\n",
    "binary_train, binary_test = train_test_split(df, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Define the type of training task. Binary classification: BINARY; Regression: REG\n",
    "task_types = [\"BINARY\"] #, \"REG\"]\n",
    "task_type = task_types[0]\n",
    "\n",
    "# Select the downloaded training and test dataset\n",
    "#if task_type == \"BINARY\":\n",
    "#    dataset_path = \"dataset/binary_classification/\"\n",
    "#    train = binary_train[0]\n",
    "#    test = binary_test[0]\n",
    "#elif task_type == \"REG\":\n",
    "#    dataset_path = \"dataset/regression/\"\n",
    "#    train = reg_train[0]\n",
    "#    test = reg_test[0]\n",
    "\n",
    "#data_train = load_svmlight_file(dataset_path + train, zero_based=False)\n",
    "#data_test = load_svmlight_file(dataset_path + test, zero_based=False)\n",
    "\n",
    "print(\"Task type selected is: \" + task_type)\n",
    "#print(\"Training dataset is: \" + binary_train)\n",
    "#print(\"Test dataset is: \" + binary_test)\n",
    "# drop rows with missing values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the tabular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension of the dataset: 14\n",
      "Size of the trainset: 239\n",
      "Size of the testset: 60\n",
      "Task type: BINARY\n",
      "Train: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52370/3239953632.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_train[y_train == -1] = 0\n",
      "/tmp/ipykernel_52370/3239953632.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y_test[y_test == -1] = 0\n"
     ]
    }
   ],
   "source": [
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
    "        label = self.labels[idx]\n",
    "        data = self.data[idx, :]\n",
    "        sample = {0: data, 1: label}\n",
    "        return sample\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "# X values all values except column num\n",
    "# y values column num\n",
    "X_train = binary_train.drop(columns=['num'])\n",
    "y_train = binary_train['num']\n",
    "\n",
    "X_test = binary_test.drop(columns=['num'])\n",
    "y_test = binary_test['num']\n",
    "\n",
    "#X_train = data_train[0].toarray()\n",
    "#y_train = data_train[1]\n",
    "#X_test = data_test[0].toarray()\n",
    "#y_test = data_test[1]\n",
    "#X_train.flags.writeable = True\n",
    "#y_train.flags.writeable = True\n",
    "#X_test.flags.writeable = True\n",
    "#y_test.flags.writeable = True\n",
    "\n",
    "# If the feature dimensions of the trainset and testset do not agree,\n",
    "# specify n_features in the load_svmlight_file function in the above cell.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
    "print(\"Feature dimension of the dataset:\", X_train.shape[1])\n",
    "print(\"Size of the trainset:\", X_train.shape[0])\n",
    "print(\"Size of the testset:\", X_test.shape[0])\n",
    "assert X_train.shape[1] == X_test.shape[1]\n",
    "print(\"Task type:\", task_type)\n",
    "print(\"Train: \", )\n",
    "\n",
    "if task_type == \"BINARY\":\n",
    "    y_train[y_train == -1] = 0\n",
    "    y_test[y_test == -1] = 0\n",
    "\n",
    "trainset = TreeDataset(np.array(X_train, copy=True), np.array(y_train, copy=True))\n",
    "testset = TreeDataset(np.array(X_test, copy=True), np.array(y_test, copy=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct tabular dataset partition for Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
    ") -> DataLoader:\n",
    "    if batch_size == \"whole\":\n",
    "        batch_size = len(dataset)\n",
    "    return DataLoader(\n",
    "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
    "    )\n",
    "\n",
    "\n",
    "# https://github.com/adap/flower\n",
    "def do_fl_partitioning(\n",
    "    trainset: Dataset,\n",
    "    testset: Dataset,\n",
    "    pool_size: int,\n",
    "    batch_size: Union[int, str],\n",
    "    val_ratio: float = 0.0,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // pool_size\n",
    "    lengths = [partition_size] * pool_size\n",
    "    if sum(lengths) != len(trainset):\n",
    "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = int(len(ds) * val_ratio)\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
    "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
    "        if len_val != 0:\n",
    "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
    "        else:\n",
    "            valloaders = None\n",
    "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
    "    return trainloaders, valloaders, testloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables for Federated XGBoost Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of clients participated in the federated learning\n",
    "client_num = 4\n",
    "\n",
    "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
    "client_tree_num = 500 // client_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build global XGBoost tree for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1080216,
     "status": "ok",
     "timestamp": 1670357156788,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "d56f2821-5cd5-49ff-c5dc-f8d088eed799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global XGBoost Training Accuracy: 0.845188\n",
      "Global XGBoost Testing Accuracy: 0.866667\n",
      "XGBClassifier(alpha=5, base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, feature_types=None, gamma=5, gpu_id=None,\n",
      "              grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=125, n_jobs=None, num_parallel_tree=1,\n",
      "              predictor=None, ...)\n"
     ]
    }
   ],
   "source": [
    "global_tree = construct_tree(X_train, y_train, client_tree_num, task_type)\n",
    "preds_train = global_tree.predict(X_train)\n",
    "preds_test = global_tree.predict(X_test)\n",
    "\n",
    "if task_type == \"BINARY\":\n",
    "    result_train = accuracy_score(y_train, preds_train)\n",
    "    result_test = accuracy_score(y_test, preds_test)\n",
    "    print(\"Global XGBoost Training Accuracy: %f\" % (result_train))\n",
    "    print(\"Global XGBoost Testing Accuracy: %f\" % (result_test))\n",
    "elif task_type == \"REG\":\n",
    "    result_train = mean_squared_error(y_train, preds_train)\n",
    "    result_test = mean_squared_error(y_test, preds_test)\n",
    "    print(\"Global XGBoost Training MSE: %f\" % (result_train))\n",
    "    print(\"Global XGBoost Testing MSE: %f\" % (result_test))\n",
    "\n",
    "print(global_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate local XGBoost trees on clients for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242310,
     "status": "ok",
     "timestamp": 1670357399084,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "0739df9f-84de-4749-8de1-7bd7c6a32ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Client 0 XGBoost Training Accuracy: 0.627119\n",
      "Local Client 0 XGBoost Testing Accuracy: 0.583333\n",
      "Local Client 1 XGBoost Training Accuracy: 0.525424\n",
      "Local Client 1 XGBoost Testing Accuracy: 0.583333\n",
      "Local Client 2 XGBoost Training Accuracy: 0.474576\n",
      "Local Client 2 XGBoost Testing Accuracy: 0.583333\n",
      "Local Client 3 XGBoost Training Accuracy: 0.822581\n",
      "Local Client 3 XGBoost Testing Accuracy: 0.750000\n"
     ]
    }
   ],
   "source": [
    "client_trees_comparison = []\n",
    "trainloaders, _, testloader = do_fl_partitioning(\n",
    "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
    ")\n",
    "\n",
    "for i, trainloader in enumerate(trainloaders):\n",
    "    for local_dataset in trainloader:\n",
    "        local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
    "        tree = construct_tree(local_X_train, local_y_train, client_tree_num, task_type)\n",
    "        client_trees_comparison.append(tree)\n",
    "\n",
    "        preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
    "        preds_test = client_trees_comparison[-1].predict(X_test)\n",
    "\n",
    "        if task_type == \"BINARY\":\n",
    "            result_train = accuracy_score(local_y_train, preds_train)\n",
    "            result_test = accuracy_score(y_test, preds_test)\n",
    "            print(\"Local Client %d XGBoost Training Accuracy: %f\" % (i, result_train))\n",
    "            print(\"Local Client %d XGBoost Testing Accuracy: %f\" % (i, result_test))\n",
    "        elif task_type == \"REG\":\n",
    "            result_train = mean_squared_error(local_y_train, preds_train)\n",
    "            result_test = mean_squared_error(y_test, preds_test)\n",
    "            print(\"Local Client %d XGBoost Training MSE: %f\" % (i, result_train))\n",
    "            print(\"Local Client %d XGBoost Testing MSE: %f\" % (i, result_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Federated XGBoost\n",
    "#### Create 1D convolutional neural network on trees prediction results. \n",
    "#### 1D kernel size == client_tree_num\n",
    "#### Make the learning rate of the tree ensembles learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1670363021675,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_channel: int = 64) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        n_out = 1\n",
    "        self.task_type = task_type\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
    "        )\n",
    "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Identity = nn.Identity()\n",
    "\n",
    "        # Add weight initialization\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(\n",
    "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.ReLU(self.conv1d(x))\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.ReLU(x)\n",
    "        if self.task_type == \"BINARY\":\n",
    "            x = self.Sigmoid(self.layer_direct(x))\n",
    "        elif self.task_type == \"REG\":\n",
    "            x = self.Identity(self.layer_direct(x))\n",
    "        return x\n",
    "\n",
    "    def get_weights(self) -> fl.common.NDArrays:\n",
    "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
    "        return [\n",
    "            np.array(val.cpu().numpy(), copy=True)\n",
    "            for _, val in self.state_dict().items()\n",
    "        ]\n",
    "\n",
    "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
    "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
    "        layer_dict = {}\n",
    "        for k, v in zip(self.state_dict().keys(), weights):\n",
    "            if v.ndim != 0:\n",
    "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
    "        state_dict = OrderedDict(layer_dict)\n",
    "        self.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    trainloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_iterations: int,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    # Define loss and optimizer\n",
    "    if task_type == \"BINARY\":\n",
    "        criterion = nn.BCELoss()\n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "    def cycle(iterable):\n",
    "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
    "        while True:\n",
    "            for x in iterable:\n",
    "                yield x\n",
    "\n",
    "    # Train the network\n",
    "    net.train()\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    pbar = (\n",
    "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
    "        if log_progress\n",
    "        else iter(cycle(trainloader))\n",
    "    )\n",
    "\n",
    "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
    "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
    "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
    "    for i, data in zip(range(num_iterations), pbar):\n",
    "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(tree_outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Collected training loss and accuracy statistics\n",
    "        total_loss += loss.item()\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        if task_type == \"BINARY\":\n",
    "            acc = Accuracy(task=\"binary\")(outputs, labels.type(torch.int))\n",
    "            total_result += acc * labels.size(0)\n",
    "        elif task_type == \"REG\":\n",
    "            mse = MeanSquaredError()(outputs, labels.type(torch.int))\n",
    "            total_result += mse * labels.size(0)\n",
    "\n",
    "        if log_progress:\n",
    "            if task_type == \"BINARY\":\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": total_loss / n_samples,\n",
    "                        \"train_acc\": total_result / n_samples,\n",
    "                    }\n",
    "                )\n",
    "            elif task_type == \"REG\":\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"train_loss\": total_loss / n_samples,\n",
    "                        \"train_mse\": total_result / n_samples,\n",
    "                    }\n",
    "                )\n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
    "\n",
    "\n",
    "def test(\n",
    "    task_type: str,\n",
    "    net: CNN,\n",
    "    testloader: DataLoader,\n",
    "    device: torch.device,\n",
    "    log_progress: bool = True,\n",
    ") -> Tuple[float, float, int]:\n",
    "    \"\"\"Evaluates the network on test data.\"\"\"\n",
    "    if task_type == \"BINARY\":\n",
    "        criterion = nn.BCELoss()\n",
    "    elif task_type == \"REG\":\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
    "        for data in pbar:\n",
    "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(tree_outputs)\n",
    "\n",
    "            # Collected testing loss and accuracy statistics\n",
    "            total_loss += criterion(outputs, labels).item()\n",
    "            n_samples += labels.size(0)\n",
    "\n",
    "            if task_type == \"BINARY\":\n",
    "                acc = Accuracy(task=\"binary\")(\n",
    "                    outputs.cpu(), labels.type(torch.int).cpu()\n",
    "                )\n",
    "                total_result += acc * labels.size(0)\n",
    "            elif task_type == \"REG\":\n",
    "                mse = MeanSquaredError()(outputs.cpu(), labels.type(torch.int).cpu())\n",
    "                total_result += mse * labels.size(0)\n",
    "\n",
    "    if log_progress:\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return total_loss / n_samples, total_result / n_samples, n_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flower custom client\n",
    "## Import Flower custom client relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower client\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetPropertiesIns,\n",
    "    GetPropertiesRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Status,\n",
    "    Code,\n",
    "    parameters_to_ndarrays,\n",
    "    ndarrays_to_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1670363021676,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    }
   },
   "outputs": [],
   "source": [
    "def tree_encoding_loader(\n",
    "    dataloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_trees: Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ],\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> DataLoader:\n",
    "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
    "    if encoding is None:\n",
    "        return None\n",
    "    data, labels = encoding\n",
    "    tree_dataset = TreeDataset(data, labels)\n",
    "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
    "\n",
    "\n",
    "class FL_Client(fl.client.Client):\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_type: str,\n",
    "        trainloader: DataLoader,\n",
    "        valloader: DataLoader,\n",
    "        client_tree_num: int,\n",
    "        client_num: int,\n",
    "        cid: str,\n",
    "        log_progress: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Creates a client for training `network.Net` on tabular dataset.\n",
    "        \"\"\"\n",
    "        self.task_type = task_type\n",
    "        self.cid = cid\n",
    "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
    "        self.trainloader_original = trainloader\n",
    "        self.valloader_original = valloader\n",
    "        self.trainloader = None\n",
    "        self.valloader = None\n",
    "        self.client_tree_num = client_tree_num\n",
    "        self.client_num = client_num\n",
    "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
    "        self.log_progress = log_progress\n",
    "\n",
    "        # instantiate model\n",
    "        self.net = CNN()\n",
    "\n",
    "        # determine device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
    "        return GetPropertiesRes(properties=self.properties)\n",
    "\n",
    "    def get_parameters(\n",
    "        self, ins: GetParametersIns\n",
    "    ) -> Tuple[\n",
    "        GetParametersRes, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
    "    ]:\n",
    "        return [\n",
    "            GetParametersRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
    "            ),\n",
    "            (self.tree, int(self.cid)),\n",
    "        ]\n",
    "\n",
    "    def set_parameters(\n",
    "        self,\n",
    "        parameters: Tuple[\n",
    "            Parameters,\n",
    "            Union[\n",
    "                Tuple[XGBClassifier, int],\n",
    "                Tuple[XGBRegressor, int],\n",
    "                List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "            ],\n",
    "        ],\n",
    "    ) -> Union[\n",
    "        Tuple[XGBClassifier, int],\n",
    "        Tuple[XGBRegressor, int],\n",
    "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "    ]:\n",
    "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
    "        return parameters[1]\n",
    "\n",
    "    def fit(self, fit_params: FitIns) -> FitRes:\n",
    "        # Process incoming request to train\n",
    "        num_iterations = fit_params.config[\"num_iterations\"]\n",
    "        batch_size = fit_params.config[\"batch_size\"]\n",
    "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
    "\n",
    "        if type(aggregated_trees) is list:\n",
    "            print(\"Client \" + self.cid + \": recieved\", len(aggregated_trees), \"trees\")\n",
    "        else:\n",
    "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
    "        self.trainloader = tree_encoding_loader(\n",
    "            self.trainloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "        self.valloader = tree_encoding_loader(\n",
    "            self.valloader_original,\n",
    "            batch_size,\n",
    "            aggregated_trees,\n",
    "            self.client_tree_num,\n",
    "            self.client_num,\n",
    "        )\n",
    "\n",
    "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
    "        num_iterations = num_iterations or len(self.trainloader)\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
    "        self.net.to(self.device)\n",
    "        train_loss, train_result, num_examples = train(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.trainloader,\n",
    "            device=self.device,\n",
    "            num_iterations=num_iterations,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "        print(\n",
    "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
    "        )\n",
    "\n",
    "        # Return training information: model, number of examples processed and metrics\n",
    "        if self.task_type == \"BINARY\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"accuracy\": train_result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            return FitRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                parameters=self.get_parameters(fit_params.config),\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
    "            )\n",
    "\n",
    "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
    "        # Process incoming request to evaluate\n",
    "        self.set_parameters(eval_params.parameters)\n",
    "\n",
    "        # Evaluate the model\n",
    "        self.net.to(self.device)\n",
    "        loss, result, num_examples = test(\n",
    "            self.task_type,\n",
    "            self.net,\n",
    "            self.valloader,\n",
    "            device=self.device,\n",
    "            log_progress=self.log_progress,\n",
    "        )\n",
    "\n",
    "        # Return evaluation information\n",
    "        if self.task_type == \"BINARY\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, accuracy={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"accuracy\": result},\n",
    "            )\n",
    "        elif self.task_type == \"REG\":\n",
    "            print(\n",
    "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
    "            )\n",
    "            return EvaluateRes(\n",
    "                status=Status(Code.OK, \"\"),\n",
    "                loss=loss,\n",
    "                num_examples=num_examples,\n",
    "                metrics={\"mse\": result},\n",
    "            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flower custom server\n",
    "## Import Flower custom server relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flower server\n",
    "import functools\n",
    "from flwr.server.strategy import FedXgbNnAvg\n",
    "from flwr.server.app import ServerConfig\n",
    "\n",
    "import timeit\n",
    "from logging import DEBUG, INFO\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from flwr.common import DisconnectRes, Parameters, ReconnectIns, Scalar\n",
    "from flwr.common.logger import log\n",
    "from flwr.common.typing import GetParametersIns\n",
    "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.history import History\n",
    "from flwr.server.strategy import Strategy\n",
    "from flwr.server.server import (\n",
    "    reconnect_clients,\n",
    "    reconnect_client,\n",
    "    fit_clients,\n",
    "    fit_client,\n",
    "    _handle_finished_future_after_fit,\n",
    "    evaluate_clients,\n",
    "    evaluate_client,\n",
    "    _handle_finished_future_after_evaluate,\n",
    ")\n",
    "\n",
    "FitResultsAndFailures = Tuple[\n",
    "    List[Tuple[ClientProxy, FitRes]],\n",
    "    List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "]\n",
    "EvaluateResultsAndFailures = Tuple[\n",
    "    List[Tuple[ClientProxy, EvaluateRes]],\n",
    "    List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FL_Server(fl.server.Server):\n",
    "    \"\"\"Flower server.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, client_manager: ClientManager, strategy: Optional[Strategy] = None\n",
    "    ) -> None:\n",
    "        self._client_manager: ClientManager = client_manager\n",
    "        self.parameters: Parameters = Parameters(\n",
    "            tensors=[], tensor_type=\"numpy.ndarray\"\n",
    "        )\n",
    "        self.strategy: Strategy = strategy\n",
    "        self.max_workers: Optional[int] = None\n",
    "\n",
    "    # pylint: disable=too-many-locals\n",
    "    def fit(self, num_rounds: int, timeout: Optional[float]) -> History:\n",
    "        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n",
    "        history = History()\n",
    "\n",
    "        # Initialize parameters\n",
    "        log(INFO, \"Initializing global parameters\")\n",
    "        self.parameters = self._get_initial_parameters(timeout=timeout)\n",
    "\n",
    "        log(INFO, \"Evaluating initial parameters\")\n",
    "        res = self.strategy.evaluate(0, parameters=self.parameters)\n",
    "        if res is not None:\n",
    "            log(\n",
    "                INFO,\n",
    "                \"initial parameters (loss, other metrics): %s, %s\",\n",
    "                res[0],\n",
    "                res[1],\n",
    "            )\n",
    "            history.add_loss_centralized(server_round=0, loss=res[0])\n",
    "            history.add_metrics_centralized(server_round=0, metrics=res[1])\n",
    "\n",
    "        # Run federated learning for num_rounds\n",
    "        log(INFO, \"FL starting\")\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        for current_round in range(1, num_rounds + 1):\n",
    "            # Train model and replace previous global model\n",
    "            res_fit = self.fit_round(server_round=current_round, timeout=timeout)\n",
    "            if res_fit:\n",
    "                parameters_prime, _, _ = res_fit  # fit_metrics_aggregated\n",
    "                if parameters_prime:\n",
    "                    self.parameters = parameters_prime\n",
    "\n",
    "            # Evaluate model using strategy implementation\n",
    "            res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)\n",
    "            if res_cen is not None:\n",
    "                loss_cen, metrics_cen = res_cen\n",
    "                log(\n",
    "                    INFO,\n",
    "                    \"fit progress: (%s, %s, %s, %s)\",\n",
    "                    current_round,\n",
    "                    loss_cen,\n",
    "                    metrics_cen,\n",
    "                    timeit.default_timer() - start_time,\n",
    "                )\n",
    "                history.add_loss_centralized(server_round=current_round, loss=loss_cen)\n",
    "                history.add_metrics_centralized(\n",
    "                    server_round=current_round, metrics=metrics_cen\n",
    "                )\n",
    "\n",
    "            # Evaluate model on a sample of available clients\n",
    "            res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
    "            if res_fed:\n",
    "                loss_fed, evaluate_metrics_fed, _ = res_fed\n",
    "                if loss_fed:\n",
    "                    history.add_loss_distributed(\n",
    "                        server_round=current_round, loss=loss_fed\n",
    "                    )\n",
    "                    history.add_metrics_distributed(\n",
    "                        server_round=current_round, metrics=evaluate_metrics_fed\n",
    "                    )\n",
    "\n",
    "        # Bookkeeping\n",
    "        end_time = timeit.default_timer()\n",
    "        elapsed = end_time - start_time\n",
    "        log(INFO, \"FL finished in %s\", elapsed)\n",
    "        return history\n",
    "\n",
    "    def evaluate_round(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        timeout: Optional[float],\n",
    "    ) -> Optional[\n",
    "        Tuple[Optional[float], Dict[str, Scalar], EvaluateResultsAndFailures]\n",
    "    ]:\n",
    "        \"\"\"Validate current global model on a number of clients.\"\"\"\n",
    "\n",
    "        # Get clients and their respective instructions from strategy\n",
    "        client_instructions = self.strategy.configure_evaluate(\n",
    "            server_round=server_round,\n",
    "            parameters=self.parameters,\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "        if not client_instructions:\n",
    "            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n",
    "            return None\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n",
    "            server_round,\n",
    "            len(client_instructions),\n",
    "            self._client_manager.num_available(),\n",
    "        )\n",
    "\n",
    "        # Collect `evaluate` results from all clients participating in this round\n",
    "        results, failures = evaluate_clients(\n",
    "            client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"evaluate_round %s received %s results and %s failures\",\n",
    "            server_round,\n",
    "            len(results),\n",
    "            len(failures),\n",
    "        )\n",
    "\n",
    "        # Aggregate the evaluation results\n",
    "        aggregated_result: Tuple[\n",
    "            Optional[float],\n",
    "            Dict[str, Scalar],\n",
    "        ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
    "\n",
    "        loss_aggregated, metrics_aggregated = aggregated_result\n",
    "        return loss_aggregated, metrics_aggregated, (results, failures)\n",
    "\n",
    "    def fit_round(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        timeout: Optional[float],\n",
    "    ) -> Optional[\n",
    "        Tuple[\n",
    "            Optional[\n",
    "                Tuple[\n",
    "                    Parameters,\n",
    "                    Union[\n",
    "                        Tuple[XGBClassifier, int],\n",
    "                        Tuple[XGBRegressor, int],\n",
    "                        List[\n",
    "                            Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
    "                        ],\n",
    "                    ],\n",
    "                ]\n",
    "            ],\n",
    "            Dict[str, Scalar],\n",
    "            FitResultsAndFailures,\n",
    "        ]\n",
    "    ]:\n",
    "        \"\"\"Perform a single round of federated averaging.\"\"\"\n",
    "\n",
    "        # Get clients and their respective instructions from strategy\n",
    "        client_instructions = self.strategy.configure_fit(\n",
    "            server_round=server_round,\n",
    "            parameters=self.parameters,\n",
    "            client_manager=self._client_manager,\n",
    "        )\n",
    "\n",
    "        if not client_instructions:\n",
    "            log(INFO, \"fit_round %s: no clients selected, cancel\", server_round)\n",
    "            return None\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"fit_round %s: strategy sampled %s clients (out of %s)\",\n",
    "            server_round,\n",
    "            len(client_instructions),\n",
    "            self._client_manager.num_available(),\n",
    "        )\n",
    "\n",
    "        # Collect `fit` results from all clients participating in this round\n",
    "        results, failures = fit_clients(\n",
    "            client_instructions=client_instructions,\n",
    "            max_workers=self.max_workers,\n",
    "            timeout=timeout,\n",
    "        )\n",
    "\n",
    "        log(\n",
    "            DEBUG,\n",
    "            \"fit_round %s received %s results and %s failures\",\n",
    "            server_round,\n",
    "            len(results),\n",
    "            len(failures),\n",
    "        )\n",
    "\n",
    "        # Aggregate training results\n",
    "        NN_aggregated: Parameters\n",
    "        trees_aggregated: Union[\n",
    "            Tuple[XGBClassifier, int],\n",
    "            Tuple[XGBRegressor, int],\n",
    "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "        ]\n",
    "        metrics_aggregated: Dict[str, Scalar]\n",
    "        aggregated, metrics_aggregated = self.strategy.aggregate_fit(\n",
    "            server_round, results, failures\n",
    "        )\n",
    "        NN_aggregated, trees_aggregated = aggregated[0], aggregated[1]\n",
    "\n",
    "        if type(trees_aggregated) is list:\n",
    "            print(\"Server side aggregated\", len(trees_aggregated), \"trees.\")\n",
    "        else:\n",
    "            print(\"Server side did not aggregate trees.\")\n",
    "\n",
    "        return (\n",
    "            [NN_aggregated, trees_aggregated],\n",
    "            metrics_aggregated,\n",
    "            (results, failures),\n",
    "        )\n",
    "\n",
    "    def _get_initial_parameters(\n",
    "        self, timeout: Optional[float]\n",
    "    ) -> Tuple[Parameters, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]]:\n",
    "        \"\"\"Get initial parameters from one of the available clients.\"\"\"\n",
    "\n",
    "        # Server-side parameter initialization\n",
    "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
    "            client_manager=self._client_manager\n",
    "        )\n",
    "        if parameters is not None:\n",
    "            log(INFO, \"Using initial parameters provided by strategy\")\n",
    "            return parameters\n",
    "\n",
    "        # Get initial parameters from one of the clients\n",
    "        log(INFO, \"Requesting initial parameters from one random client\")\n",
    "        random_client = self._client_manager.sample(1)[0]\n",
    "        ins = GetParametersIns(config={})\n",
    "        get_parameters_res_tree = random_client.get_parameters(ins=ins, timeout=timeout)\n",
    "        parameters = [get_parameters_res_tree[0].parameters, get_parameters_res_tree[1]]\n",
    "        log(INFO, \"Received initial parameters from one random client\")\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create server-side evaluation and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1670363021676,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    }
   },
   "outputs": [],
   "source": [
    "def print_model_layers(model: nn.Module) -> None:\n",
    "    print(model)\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "\n",
    "def serverside_eval(\n",
    "    server_round: int,\n",
    "    parameters: Tuple[\n",
    "        Parameters,\n",
    "        Union[\n",
    "            Tuple[XGBClassifier, int],\n",
    "            Tuple[XGBRegressor, int],\n",
    "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
    "        ],\n",
    "    ],\n",
    "    config: Dict[str, Scalar],\n",
    "    task_type: str,\n",
    "    testloader: DataLoader,\n",
    "    batch_size: int,\n",
    "    client_tree_num: int,\n",
    "    client_num: int,\n",
    ") -> Tuple[float, Dict[str, float]]:\n",
    "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = \"cpu\"\n",
    "    model = CNN()\n",
    "    # print_model_layers(model)\n",
    "\n",
    "    model.set_weights(parameters_to_ndarrays(parameters[0]))\n",
    "    model.to(device)\n",
    "\n",
    "    trees_aggregated = parameters[1]\n",
    "    testloader = tree_encoding_loader(\n",
    "        testloader, batch_size, trees_aggregated, client_tree_num, client_num\n",
    "    )\n",
    "    loss, result, _ = test(\n",
    "        task_type, model, testloader, device=device, log_progress=False\n",
    "    )\n",
    "\n",
    "    if task_type == \"BINARY\":\n",
    "        print(\n",
    "            f\"Evaluation on the server: test_loss={loss:.4f}, test_accuracy={result:.4f}\"\n",
    "        )\n",
    "        return loss, {\"accuracy\": result}\n",
    "    elif task_type == \"REG\":\n",
    "        print(f\"Evaluation on the server: test_loss={loss:.4f}, test_mse={result:.4f}\")\n",
    "        return loss, {\"mse\": result}\n",
    "\n",
    "\n",
    "def start_experiment(\n",
    "    task_type: str,\n",
    "    trainset: Dataset,\n",
    "    testset: Dataset,\n",
    "    num_rounds: int = 5,\n",
    "    client_tree_num: int = 50,\n",
    "    client_pool_size: int = 5,\n",
    "    num_iterations: int = 100,\n",
    "    fraction_fit: float = 1.0,\n",
    "    min_fit_clients: int = 2,\n",
    "    batch_size: int = 32,\n",
    "    val_ratio: float = 0.1,\n",
    ") -> History:\n",
    "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
    "\n",
    "    # Partition the dataset into subsets reserved for each client.\n",
    "    # - 'val_ratio' controls the proportion of the (local) client reserved as a local test set\n",
    "    # (good for testing how the final model performs on the client's local unseen data)\n",
    "    trainloaders, valloaders, testloader = do_fl_partitioning(\n",
    "        trainset,\n",
    "        testset,\n",
    "        batch_size=\"whole\",\n",
    "        pool_size=client_pool_size,\n",
    "        val_ratio=val_ratio,\n",
    "    )\n",
    "    print(\n",
    "        f\"Data partitioned across {client_pool_size} clients\"\n",
    "        f\" and {val_ratio} of local dataset reserved for validation.\"\n",
    "    )\n",
    "\n",
    "    # Configure the strategy\n",
    "    def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
    "        print(f\"Configuring round {server_round}\")\n",
    "        return {\n",
    "            \"num_iterations\": num_iterations,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "\n",
    "    # FedXgbNnAvg\n",
    "    strategy = FedXgbNnAvg(\n",
    "        fraction_fit=fraction_fit,\n",
    "        fraction_evaluate=fraction_fit if val_ratio > 0.0 else 0.0,\n",
    "        min_fit_clients=min_fit_clients,\n",
    "        min_evaluate_clients=min_fit_clients,\n",
    "        min_available_clients=client_pool_size,  # all clients should be available\n",
    "        on_fit_config_fn=fit_config,\n",
    "        on_evaluate_config_fn=(lambda r: {\"batch_size\": batch_size}),\n",
    "        evaluate_fn=functools.partial(\n",
    "            serverside_eval,\n",
    "            task_type=task_type,\n",
    "            testloader=testloader,\n",
    "            batch_size=batch_size,\n",
    "            client_tree_num=client_tree_num,\n",
    "            client_num=client_num,\n",
    "        ),\n",
    "        accept_failures=False,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} client in the pool.\"\n",
    "    )\n",
    "    print(\n",
    "        f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\"\n",
    "    )\n",
    "\n",
    "    def client_fn(cid: str) -> fl.client.Client:\n",
    "        \"\"\"Creates a federated learning client\"\"\"\n",
    "        if val_ratio > 0.0 and val_ratio <= 1.0:\n",
    "            return FL_Client(\n",
    "                task_type,\n",
    "                trainloaders[int(cid)],\n",
    "                valloaders[int(cid)],\n",
    "                client_tree_num,\n",
    "                client_pool_size,\n",
    "                cid,\n",
    "                log_progress=False,\n",
    "            )\n",
    "        else:\n",
    "            return FL_Client(\n",
    "                task_type,\n",
    "                trainloaders[int(cid)],\n",
    "                None,\n",
    "                client_tree_num,\n",
    "                client_pool_size,\n",
    "                cid,\n",
    "                log_progress=False,\n",
    "            )\n",
    "\n",
    "    # Start the simulation\n",
    "    history = fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        server=FL_Server(client_manager=SimpleClientManager(), strategy=strategy),\n",
    "        num_clients=client_pool_size,\n",
    "        client_resources=client_resources,\n",
    "        config=ServerConfig(num_rounds=num_rounds),\n",
    "        strategy=strategy,\n",
    "    )\n",
    "\n",
    "    print(history)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start federated training and inference\n",
    "#### High-level workflow: \n",
    "#### At round 1, each client first builds their own local XGBoost tree, and sends to the server. The server aggregates all trees and sends to all clients. \n",
    "#### After round 1, each client calculates every other client tree’s prediction results, and trains a convolutional neural network with 1D convolution kernel size == the number of XGBoost trees in the tree ensemble. \n",
    "#### The sharing of privacy-sensitive information is not needed, and the learning rate (a hyperparameter for XGBoost) is learnable using 1D convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "executionInfo": {
     "elapsed": 7610,
     "status": "error",
     "timestamp": 1670363029252,
     "user": {
      "displayName": "Chenyang Ma",
      "userId": "17975430055716133031"
     },
     "user_tz": 0
    },
    "outputId": "ee2b7146-07ec-4f97-ba44-5b12b35bbeaf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING flwr 2023-08-08 13:35:48,570 | app.py:210 | Both server and strategy were provided, ignoring strategy\n",
      "INFO flwr 2023-08-08 13:35:48,573 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data partitioned across 4 clients and 0.0 of local dataset reserved for validation.\n",
      "FL experiment configured for 20 rounds with 4 client in the pool.\n",
      "FL round will proceed with 100.0% of clients sampled, at least 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 13:35:52,839\tINFO worker.py:1636 -- Started a local Ray instance.\n",
      "INFO flwr 2023-08-08 13:35:53,296 | app.py:179 | Flower VCE: Ray initialized with resources: {'CPU': 32.0, 'memory': 17022976820.0, 'GPU': 1.0, 'accelerator_type:G': 1.0, 'object_store_memory': 8511488409.0, 'node:172.20.106.81': 1.0}\n",
      "INFO flwr 2023-08-08 13:35:53,296 | 2437557820.py:20 | Initializing global parameters\n",
      "INFO flwr 2023-08-08 13:35:53,297 | 2437557820.py:226 | Requesting initial parameters from one random client\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=60715)\u001b[0m /home/w.lindskog/.cache/pypoetry/virtualenvs/test-wA6o1Upo-py3.9/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:171: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=60715)\u001b[0m   return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n",
      "INFO flwr 2023-08-08 13:35:54,345 | 2437557820.py:231 | Received initial parameters from one random client\n",
      "INFO flwr 2023-08-08 13:35:54,345 | 2437557820.py:23 | Evaluating initial parameters\n",
      "INFO flwr 2023-08-08 13:35:55,139 | 2437557820.py:26 | initial parameters (loss, other metrics): 0.046136924624443056, {'accuracy': tensor(0.7500)}\n",
      "INFO flwr 2023-08-08 13:35:55,140 | 2437557820.py:36 | FL starting\n",
      "DEBUG flwr 2023-08-08 13:35:55,140 | 2437557820.py:165 | fit_round 1: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0461, test_accuracy=0.7500\n",
      "Configuring round 1\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 0: only had its own tree\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 0: training for 100 iterations/updates\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 0: training round complete, 1475 examples processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:35:56,704 | 2437557820.py:180 | fit_round 1 received 4 results and 0 failures\n",
      "WARNING flwr 2023-08-08 13:35:56,705 | fedxgb_nn_avg.py:88 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:35:57,483 | 2437557820.py:51 | fit progress: (1, 0.04620380699634552, {'accuracy': tensor(0.4167)}, 2.3430362759972923)\n",
      "INFO flwr 2023-08-08 13:35:57,484 | 2437557820.py:98 | evaluate_round 1: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:35:57,484 | 2437557820.py:165 | fit_round 2: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0462, test_accuracy=0.4167\n",
      "Configuring round 2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 2: recieved 4 trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:35:57,923 | 2437557820.py:180 | fit_round 2 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:35:58,454 | 2437557820.py:51 | fit progress: (2, 0.046172961592674255, {'accuracy': tensor(0.5833)}, 3.3141753500094637)\n",
      "INFO flwr 2023-08-08 13:35:58,455 | 2437557820.py:98 | evaluate_round 2: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:35:58,455 | 2437557820.py:165 | fit_round 3: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0462, test_accuracy=0.5833\n",
      "Configuring round 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:35:58,892 | 2437557820.py:180 | fit_round 3 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:35:59,700 | 2437557820.py:51 | fit progress: (3, 0.04619357387224833, {'accuracy': tensor(0.5833)}, 4.560227320995182)\n",
      "INFO flwr 2023-08-08 13:35:59,701 | 2437557820.py:98 | evaluate_round 3: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:35:59,701 | 2437557820.py:165 | fit_round 4: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0462, test_accuracy=0.5833\n",
      "Configuring round 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:00,141 | 2437557820.py:180 | fit_round 4 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:00,564 | 2437557820.py:51 | fit progress: (4, 0.046111659208933516, {'accuracy': tensor(0.5833)}, 5.423948878014926)\n",
      "INFO flwr 2023-08-08 13:36:00,565 | 2437557820.py:98 | evaluate_round 4: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:00,565 | 2437557820.py:165 | fit_round 5: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0461, test_accuracy=0.5833\n",
      "Configuring round 5\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 2: only had its own tree\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 3: training for 100 iterations/updates\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 3: training round complete, 1550 examples processed\u001b[32m [repeated 15x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:01,013 | 2437557820.py:180 | fit_round 5 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:01,579 | 2437557820.py:51 | fit progress: (5, 0.04617087741692861, {'accuracy': tensor(0.5833)}, 6.43919243500568)\n",
      "INFO flwr 2023-08-08 13:36:01,580 | 2437557820.py:98 | evaluate_round 5: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:01,580 | 2437557820.py:165 | fit_round 6: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0462, test_accuracy=0.5833\n",
      "Configuring round 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:02,029 | 2437557820.py:180 | fit_round 6 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:02,579 | 2437557820.py:51 | fit progress: (6, 0.04599355657895406, {'accuracy': tensor(0.5833)}, 7.439267558977008)\n",
      "INFO flwr 2023-08-08 13:36:02,580 | 2437557820.py:98 | evaluate_round 6: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:02,580 | 2437557820.py:165 | fit_round 7: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0460, test_accuracy=0.5833\n",
      "Configuring round 7\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 1: recieved 4 trees\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:03,024 | 2437557820.py:180 | fit_round 7 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:03,573 | 2437557820.py:51 | fit progress: (7, 0.0460891713698705, {'accuracy': tensor(0.5833)}, 8.43261195498053)\n",
      "INFO flwr 2023-08-08 13:36:03,573 | 2437557820.py:98 | evaluate_round 7: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:03,574 | 2437557820.py:165 | fit_round 8: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0461, test_accuracy=0.5833\n",
      "Configuring round 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:04,015 | 2437557820.py:180 | fit_round 8 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:04,633 | 2437557820.py:51 | fit progress: (8, 0.0460180660088857, {'accuracy': tensor(0.5833)}, 9.492457629996352)\n",
      "INFO flwr 2023-08-08 13:36:04,633 | 2437557820.py:98 | evaluate_round 8: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:04,634 | 2437557820.py:165 | fit_round 9: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0460, test_accuracy=0.5833\n",
      "Configuring round 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:05,072 | 2437557820.py:180 | fit_round 9 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:05,977 | 2437557820.py:51 | fit progress: (9, 0.045807592074076336, {'accuracy': tensor(0.5833)}, 10.837176255998202)\n",
      "INFO flwr 2023-08-08 13:36:05,978 | 2437557820.py:98 | evaluate_round 9: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:05,978 | 2437557820.py:165 | fit_round 10: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "Configuring round 10\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 1: training for 100 iterations/updates\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 1: training round complete, 1475 examples processed\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:06,426 | 2437557820.py:180 | fit_round 10 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:07,284 | 2437557820.py:51 | fit progress: (10, 0.04593476553757985, {'accuracy': tensor(0.5833)}, 12.143652943021152)\n",
      "INFO flwr 2023-08-08 13:36:07,284 | 2437557820.py:98 | evaluate_round 10: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:07,285 | 2437557820.py:165 | fit_round 11: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0459, test_accuracy=0.5833\n",
      "Configuring round 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:07,735 | 2437557820.py:180 | fit_round 11 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 3: recieved 4 trees\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:08,145 | 2437557820.py:51 | fit progress: (11, 0.045953636368115745, {'accuracy': tensor(0.5833)}, 13.00453059800202)\n",
      "INFO flwr 2023-08-08 13:36:08,145 | 2437557820.py:98 | evaluate_round 11: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:08,146 | 2437557820.py:165 | fit_round 12: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0460, test_accuracy=0.5833\n",
      "Configuring round 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:08,592 | 2437557820.py:180 | fit_round 12 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:08,966 | 2437557820.py:51 | fit progress: (12, 0.04572396079699199, {'accuracy': tensor(0.5833)}, 13.826382289000321)\n",
      "INFO flwr 2023-08-08 13:36:08,967 | 2437557820.py:98 | evaluate_round 12: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:08,967 | 2437557820.py:165 | fit_round 13: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0457, test_accuracy=0.5833\n",
      "Configuring round 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:09,412 | 2437557820.py:180 | fit_round 13 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:09,980 | 2437557820.py:51 | fit progress: (13, 0.04578303694725037, {'accuracy': tensor(0.5833)}, 14.839786958007608)\n",
      "INFO flwr 2023-08-08 13:36:09,981 | 2437557820.py:98 | evaluate_round 13: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:09,981 | 2437557820.py:165 | fit_round 14: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "Configuring round 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:10,422 | 2437557820.py:180 | fit_round 14 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:11,151 | 2437557820.py:51 | fit progress: (14, 0.04591499467690786, {'accuracy': tensor(0.5833)}, 16.01114628597861)\n",
      "INFO flwr 2023-08-08 13:36:11,152 | 2437557820.py:98 | evaluate_round 14: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:11,152 | 2437557820.py:165 | fit_round 15: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0459, test_accuracy=0.5833\n",
      "Configuring round 15\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 2: training for 100 iterations/updates\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 2: training round complete, 1475 examples processed\u001b[32m [repeated 20x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:11,594 | 2437557820.py:180 | fit_round 15 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:12,787 | 2437557820.py:51 | fit progress: (15, 0.045831435918807985, {'accuracy': tensor(0.5833)}, 17.647341680014506)\n",
      "INFO flwr 2023-08-08 13:36:12,788 | 2437557820.py:98 | evaluate_round 15: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:12,789 | 2437557820.py:165 | fit_round 16: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "Configuring round 16\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 0: recieved 4 trees\u001b[32m [repeated 17x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:13,239 | 2437557820.py:180 | fit_round 16 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:13,779 | 2437557820.py:51 | fit progress: (16, 0.045706780751546223, {'accuracy': tensor(0.5833)}, 18.638598862977233)\n",
      "INFO flwr 2023-08-08 13:36:13,779 | 2437557820.py:98 | evaluate_round 16: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:13,780 | 2437557820.py:165 | fit_round 17: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0457, test_accuracy=0.5833\n",
      "Configuring round 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:14,281 | 2437557820.py:180 | fit_round 17 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:15,159 | 2437557820.py:51 | fit progress: (17, 0.045669001340866086, {'accuracy': tensor(0.5833)}, 20.0189012279734)\n",
      "INFO flwr 2023-08-08 13:36:15,160 | 2437557820.py:98 | evaluate_round 17: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:15,160 | 2437557820.py:165 | fit_round 18: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0457, test_accuracy=0.5833\n",
      "Configuring round 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:15,602 | 2437557820.py:180 | fit_round 18 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:15,978 | 2437557820.py:51 | fit progress: (18, 0.045799582203229265, {'accuracy': tensor(0.5833)}, 20.83744656800991)\n",
      "INFO flwr 2023-08-08 13:36:15,978 | 2437557820.py:98 | evaluate_round 18: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:15,978 | 2437557820.py:165 | fit_round 19: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "Configuring round 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:16,427 | 2437557820.py:180 | fit_round 19 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=60715)\u001b[0m Client 0: training for 100 iterations/updates\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=60688)\u001b[0m Client 2: training round complete, 1475 examples processed\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:16,815 | 2437557820.py:51 | fit progress: (19, 0.045791458090146384, {'accuracy': tensor(0.5833)}, 21.675183771003503)\n",
      "INFO flwr 2023-08-08 13:36:16,816 | 2437557820.py:98 | evaluate_round 19: no clients selected, cancel\n",
      "DEBUG flwr 2023-08-08 13:36:16,816 | 2437557820.py:165 | fit_round 20: strategy sampled 4 clients (out of 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "Configuring round 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-08-08 13:36:17,258 | 2437557820.py:180 | fit_round 20 received 4 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server side aggregated 4 trees.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-08-08 13:36:18,126 | 2437557820.py:51 | fit progress: (20, 0.0458189457654953, {'accuracy': tensor(0.5833)}, 22.986348402977455)\n",
      "INFO flwr 2023-08-08 13:36:18,127 | 2437557820.py:98 | evaluate_round 20: no clients selected, cancel\n",
      "INFO flwr 2023-08-08 13:36:18,127 | 2437557820.py:79 | FL finished in 22.987163305981085\n",
      "INFO flwr 2023-08-08 13:36:18,128 | app.py:225 | app_fit: losses_distributed []\n",
      "INFO flwr 2023-08-08 13:36:18,128 | app.py:226 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2023-08-08 13:36:18,128 | app.py:227 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-08-08 13:36:18,129 | app.py:228 | app_fit: losses_centralized [(0, 0.046136924624443056), (1, 0.04620380699634552), (2, 0.046172961592674255), (3, 0.04619357387224833), (4, 0.046111659208933516), (5, 0.04617087741692861), (6, 0.04599355657895406), (7, 0.0460891713698705), (8, 0.0460180660088857), (9, 0.045807592074076336), (10, 0.04593476553757985), (11, 0.045953636368115745), (12, 0.04572396079699199), (13, 0.04578303694725037), (14, 0.04591499467690786), (15, 0.045831435918807985), (16, 0.045706780751546223), (17, 0.045669001340866086), (18, 0.045799582203229265), (19, 0.045791458090146384), (20, 0.0458189457654953)]\n",
      "INFO flwr 2023-08-08 13:36:18,132 | app.py:229 | app_fit: metrics_centralized {'accuracy': [(0, tensor(0.7500)), (1, tensor(0.4167)), (2, tensor(0.5833)), (3, tensor(0.5833)), (4, tensor(0.5833)), (5, tensor(0.5833)), (6, tensor(0.5833)), (7, tensor(0.5833)), (8, tensor(0.5833)), (9, tensor(0.5833)), (10, tensor(0.5833)), (11, tensor(0.5833)), (12, tensor(0.5833)), (13, tensor(0.5833)), (14, tensor(0.5833)), (15, tensor(0.5833)), (16, tensor(0.5833)), (17, tensor(0.5833)), (18, tensor(0.5833)), (19, tensor(0.5833)), (20, tensor(0.5833))]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on the server: test_loss=0.0458, test_accuracy=0.5833\n",
      "History (loss, centralized):\n",
      "\tround 0: 0.046136924624443056\n",
      "\tround 1: 0.04620380699634552\n",
      "\tround 2: 0.046172961592674255\n",
      "\tround 3: 0.04619357387224833\n",
      "\tround 4: 0.046111659208933516\n",
      "\tround 5: 0.04617087741692861\n",
      "\tround 6: 0.04599355657895406\n",
      "\tround 7: 0.0460891713698705\n",
      "\tround 8: 0.0460180660088857\n",
      "\tround 9: 0.045807592074076336\n",
      "\tround 10: 0.04593476553757985\n",
      "\tround 11: 0.045953636368115745\n",
      "\tround 12: 0.04572396079699199\n",
      "\tround 13: 0.04578303694725037\n",
      "\tround 14: 0.04591499467690786\n",
      "\tround 15: 0.045831435918807985\n",
      "\tround 16: 0.045706780751546223\n",
      "\tround 17: 0.045669001340866086\n",
      "\tround 18: 0.045799582203229265\n",
      "\tround 19: 0.045791458090146384\n",
      "\tround 20: 0.0458189457654953\n",
      "History (metrics, centralized):\n",
      "{'accuracy': [(0, tensor(0.7500)), (1, tensor(0.4167)), (2, tensor(0.5833)), (3, tensor(0.5833)), (4, tensor(0.5833)), (5, tensor(0.5833)), (6, tensor(0.5833)), (7, tensor(0.5833)), (8, tensor(0.5833)), (9, tensor(0.5833)), (10, tensor(0.5833)), (11, tensor(0.5833)), (12, tensor(0.5833)), (13, tensor(0.5833)), (14, tensor(0.5833)), (15, tensor(0.5833)), (16, tensor(0.5833)), (17, tensor(0.5833)), (18, tensor(0.5833)), (19, tensor(0.5833)), (20, tensor(0.5833))]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, centralized):\n",
       "\tround 0: 0.046136924624443056\n",
       "\tround 1: 0.04620380699634552\n",
       "\tround 2: 0.046172961592674255\n",
       "\tround 3: 0.04619357387224833\n",
       "\tround 4: 0.046111659208933516\n",
       "\tround 5: 0.04617087741692861\n",
       "\tround 6: 0.04599355657895406\n",
       "\tround 7: 0.0460891713698705\n",
       "\tround 8: 0.0460180660088857\n",
       "\tround 9: 0.045807592074076336\n",
       "\tround 10: 0.04593476553757985\n",
       "\tround 11: 0.045953636368115745\n",
       "\tround 12: 0.04572396079699199\n",
       "\tround 13: 0.04578303694725037\n",
       "\tround 14: 0.04591499467690786\n",
       "\tround 15: 0.045831435918807985\n",
       "\tround 16: 0.045706780751546223\n",
       "\tround 17: 0.045669001340866086\n",
       "\tround 18: 0.045799582203229265\n",
       "\tround 19: 0.045791458090146384\n",
       "\tround 20: 0.0458189457654953\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, tensor(0.7500)), (1, tensor(0.4167)), (2, tensor(0.5833)), (3, tensor(0.5833)), (4, tensor(0.5833)), (5, tensor(0.5833)), (6, tensor(0.5833)), (7, tensor(0.5833)), (8, tensor(0.5833)), (9, tensor(0.5833)), (10, tensor(0.5833)), (11, tensor(0.5833)), (12, tensor(0.5833)), (13, tensor(0.5833)), (14, tensor(0.5833)), (15, tensor(0.5833)), (16, tensor(0.5833)), (17, tensor(0.5833)), (18, tensor(0.5833)), (19, tensor(0.5833)), (20, tensor(0.5833))]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_experiment(\n",
    "    task_type=task_type,\n",
    "    trainset=trainset,\n",
    "    testset=testset,\n",
    "    num_rounds=20,\n",
    "    client_tree_num=client_tree_num,\n",
    "    client_pool_size=client_num,\n",
    "    num_iterations=100,\n",
    "    batch_size=16,\n",
    "    fraction_fit=1.0,\n",
    "    min_fit_clients=1,\n",
    "    val_ratio=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "FedXGBoost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
